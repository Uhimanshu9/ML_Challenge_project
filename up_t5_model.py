# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16A4v4LPks6STL8i2BNAScyqrAkAXiQwm
"""

import pandas as pd
import numpy as np
import regex as re

df_train=pd.read_csv("ex.csv")

def cleaning(text):
    if isinstance(text, str):
        text = text.replace('[', '').replace(']', '').replace("'", '')
        text=text.encode('ascii',errors='ignore').decode()
        text=text.replace('\n','')
        text=text.replace('\t','')
        text=text.replace('/r','')
        text=text.replace('/','')
        text=text.replace('(','')
        text=text.replace(')','')
        text=text.replace('?','')
        text=text.replace('!','')
        text=text.replace('@','')
        text=text.replace('<','')
        text=text.replace('>','')

        return text
    return ''

df_train['extracted_text'] = df_train['extracted_text'].apply(cleaning)

df_train['training_input'] = df_train[['entity_name','extracted_text']].agg(' | '.join, axis=1)

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split

# Load T5 model and tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-base')
model = T5ForConditionalGeneration.from_pretrained('t5-base')

# Prepare training data
df_train['training_input'] = "Extract entity: " + df_train[['entity_name', 'extracted_text']].agg(' | '.join, axis=1)
df_train['training_output'] = df_train['entity_value']

X = df_train['training_input']
y = df_train['training_output']

# Split into train and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=104, test_size=0.10, shuffle=True)

def tokenize_function(text_list):
    return tokenizer(text_list, return_tensors="pt", padding=True, truncation=True, max_length=512)

input_tokens = [tokenize_function(x)['input_ids'].squeeze(0) for x in X_train]
target_tokens = [tokenize_function(y)['input_ids'].squeeze(0) for y in y_train]

import os
def save_checkpoint(epoch, model, optimizer, tokenizer, save_dir="./checkpoints1"):
    os.makedirs(save_dir, exist_ok=True)
    checkpoint_path = os.path.join(save_dir, f"checkpoint_epoch_{epoch}.pt")
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'tokenizer': tokenizer,
    }, checkpoint_path)
    print(f"Checkpoint saved at {checkpoint_path}")

from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm

model.train()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Move data to GPU
input_tokens = [x.to(device) for x in input_tokens]
target_tokens = [y.to(device) for y in target_tokens]

epochs = 20
batch_size = 8

for epoch in range(epochs):
    total_loss = 0
    print(f'Epoch {epoch + 1}/{epochs}')

    for i in tqdm(range(0, len(input_tokens), batch_size), desc="Training Progress", unit="batch"):
        inputs_batch = pad_sequence(input_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id)
        targets_batch = pad_sequence(target_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id)

        optimizer.zero_grad()
        outputs = model(input_ids=inputs_batch, labels=targets_batch)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1} completed. Average Loss: {total_loss / len(input_tokens)}')
    save_checkpoint(epoch + 1, model, optimizer, tokenizer)
print("Training complete")

model.eval()

input_tokens_test = [tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)['input_ids'].squeeze(0).to(device) for text in X_test]

predicted_values = []
actual_values = list(y_test)

batch_size = 8
total_bleu_score = 0
correct_predictions = 0

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
smooth_fn = SmoothingFunction().method4

for i in tqdm(range(0, len(input_tokens_test), batch_size), desc="Evaluating"):
    inputs_batch = pad_sequence(input_tokens_test[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id)

    with torch.no_grad():
        generated_ids = model.generate(inputs_batch, max_length=20)

    for idx, generated_id in enumerate(generated_ids):
        generated_text = tokenizer.decode(generated_id, skip_special_tokens=True)
        predicted_values.append(generated_text)

        actual_text = actual_values[i + idx]
        reference = [actual_text.split()]
        candidate = generated_text.split()

        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)
        total_bleu_score += bleu_score

        if generated_text == actual_text:
            correct_predictions += 1

average_bleu_score = total_bleu_score / len(predicted_values)
accuracy = correct_predictions / len(predicted_values)

print(f"\nExact Match Accuracy: {accuracy * 100:.2f}%")
print(f"Average BLEU Score: {average_bleu_score:.4f}")

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

model.eval()

input_tokens_test = [tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)['input_ids'].squeeze(0).to(device) for text in X_test]

predicted_values = []
actual_values = list(y_test)

# Batch size for evaluation
batch_size = 8

# BLEU smoothing function
smooth_fn = SmoothingFunction().method4  # Handles cases where there is no overlap in n-grams

# Initialize variables for BLEU and accuracy
total_bleu_score = 0
correct_predictions = 0

# Loop through test data in batches
for i in tqdm(range(0, len(input_tokens_test), batch_size), desc="Evaluating"):
    inputs_batch = pad_sequence(input_tokens_test[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id)

    # Generate predictions
    with torch.no_grad():
        generated_ids = model.generate(inputs_batch, max_length=20)

    # Decode predictions to text
    for idx, generated_id in enumerate(generated_ids):
        generated_text = tokenizer.decode(generated_id, skip_special_tokens=True)
        predicted_values.append(generated_text)

        # Calculate BLEU score for this prediction (reference is the actual value from y_test)
        actual_text = actual_values[i + idx]
        reference = [actual_text.split()]  # BLEU expects a list of reference sentences (each as a list of words)
        candidate = generated_text.split()  # Candidate translation

        # Calculate BLEU score for this sample
        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)
        total_bleu_score += bleu_score

        # Simple exact match comparison
        if generated_text == actual_text:
            correct_predictions += 1

# Compute average BLEU score and accuracy
average_bleu_score = total_bleu_score / len(predicted_values)
accuracy = correct_predictions / len(predicted_values)

# Print a few examples with BLEU scores
for i in range(10):
    print(f"Predicted: {predicted_values[i]}, Actual: {actual_values[i]}")
    reference = [actual_values[i].split()]
    candidate = predicted_values[i].split()
    sample_bleu = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)
    print(f"BLEU score for sample {i}: {sample_bleu:.4f}")

# Print final evaluation metrics
print(f"\nExact Match Accuracy: {accuracy * 100:.2f}%")
print(f"Average BLEU Score: {average_bleu_score:.4f}")

save_directory = "./final_models/t5_model"

# Save the model and tokenizer
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

print(f"T5 model and tokenizer saved in {save_directory}")