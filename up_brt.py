# -*- coding: utf-8 -*-
"""new_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s0CI8QgYdib_3OZcATIJ-McqtxjcWF3a
"""

# Imports
import pandas as pd
import torch
import os
from transformers import BartTokenizer, BartForConditionalGeneration
from sklearn.model_selection import train_test_split
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# Load Data
df_train = pd.read_csv("ex.csv")

# Data Cleaning Function
def cleaning(text):
    if isinstance(text, str):
        text = text.replace('[', '').replace(']', '').replace("'", '')
        text = text.encode('ascii', errors='ignore').decode()
        text = text.replace('\n', '').replace('\t', '').replace('/r', '')
        text = text.replace('/', '').replace('(', '').replace(')', '')
        text = text.replace('?', '').replace('!', '').replace('@', '')
        text = text.replace('<', '').replace('>', '')
        return text
    return ''

# Preprocess Data
df_train['extracted_text'] = df_train['extracted_text'].apply(cleaning)
df_train['training_input'] = df_train[['entity_name', 'extracted_text']].agg(' | '.join, axis=1)

# Tokenizer and Model Initialization
tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')

# Tokenization Function
def tokenize_data(text_list):
    return tokenizer(text_list, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Prepare Data for Training and Testing
X = df_train['training_input']
y = df_train['entity_value']

# Train, Validation, and Test Split
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.10, random_state=104, shuffle=True)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.10, random_state=104, shuffle=True)

# Tokenize Data
input_tokens = [tokenize_data(x)['input_ids'].squeeze(0) for x in X_train]
target_tokens = [tokenize_data(y)['input_ids'].squeeze(0) for y in y_train]
val_input_tokens = [tokenize_data(x)['input_ids'].squeeze(0) for x in X_val]
val_target_tokens = [tokenize_data(y)['input_ids'].squeeze(0) for y in y_val]

# Device Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Training and Validation Loop
epochs = 10
batch_size = 16

for epoch in range(epochs):
    # Training Phase
    model.train()
    train_total_loss = 0
    correct_train_predictions = 0
    total_train_samples = 0

    print(f"\nEpoch {epoch + 1}/{epochs}")

    for i in tqdm(range(0, len(input_tokens), batch_size), desc="Training"):
        inputs_batch = pad_sequence(input_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id).to(device)
        targets_batch = pad_sequence(target_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id).to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=inputs_batch, labels=targets_batch)
        loss = outputs.loss
        train_total_loss += loss.item()
        loss.backward()
        optimizer.step()

        # Calculate training accuracy
        with torch.no_grad():
            generated_ids = model.generate(inputs_batch, max_length=20)
            for idx, generated_id in enumerate(generated_ids):
                predicted_text = tokenizer.decode(generated_id, skip_special_tokens=True)
                actual_text = tokenizer.decode(targets_batch[idx], skip_special_tokens=True)
                if predicted_text == actual_text:
                    correct_train_predictions += 1
                total_train_samples += 1

    train_accuracy = correct_train_predictions / total_train_samples
    train_loss = train_total_loss / len(input_tokens)

    # Validation Phase
    model.eval()
    val_total_loss = 0
    correct_val_predictions = 0
    total_val_samples = 0

    for i in tqdm(range(0, len(val_input_tokens), batch_size), desc="Validation"):
        inputs_batch = pad_sequence(val_input_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id).to(device)
        targets_batch = pad_sequence(val_target_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id).to(device)

        with torch.no_grad():
            outputs = model(input_ids=inputs_batch, labels=targets_batch)
            val_total_loss += outputs.loss.item()

            # Calculate validation accuracy
            generated_ids = model.generate(inputs_batch, max_length=20)
            for idx, generated_id in enumerate(generated_ids):
                predicted_text = tokenizer.decode(generated_id, skip_special_tokens=True)
                actual_text = tokenizer.decode(targets_batch[idx], skip_special_tokens=True)
                if predicted_text == actual_text:
                    correct_val_predictions += 1
                total_val_samples += 1

    val_accuracy = correct_val_predictions / total_val_samples
    val_loss = val_total_loss / len(val_input_tokens)

    # Print Epoch Metrics
    print(f"Epoch {epoch + 1} Metrics:")
    print(f"  Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%")
    print(f"  Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%")

    # Save checkpoint
    checkpoint_dir = "./checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f"epoch_{epoch + 1}.pt"))

print("Training complete.")

# Save the Final Model
save_directory = "./final_models/bert_model_updated"
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)
print(f"Model and tokenizer saved in {save_directory}")

# BLEU Score Evaluation
print("\nEvaluating BLEU Score on Test Data...")
test_input_tokens = [tokenize_data(x)['input_ids'].squeeze(0).to(device) for x in X_test]
predicted_values = []
actual_values = list(y_test)
total_bleu_score = 0
correct_predictions = 0
smooth_fn = SmoothingFunction().method4  # Smoothing function for BLEU

for i in tqdm(range(0, len(test_input_tokens), batch_size), desc="Evaluating"):
    inputs_batch = pad_sequence(test_input_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id).to(device)

    with torch.no_grad():
        generated_ids = model.generate(inputs_batch, max_length=20)

    for idx, generated_id in enumerate(generated_ids):
        predicted_text = tokenizer.decode(generated_id, skip_special_tokens=True)
        actual_text = actual_values[i + idx]
        predicted_values.append(predicted_text)

        reference = [actual_text.split()]
        candidate = predicted_text.split()
        total_bleu_score += sentence_bleu(reference, candidate, smoothing_function=smooth_fn)
        if predicted_text == actual_text:
            correct_predictions += 1

# Test Metrics
average_test_bleu = total_bleu_score / len(predicted_values)
test_accuracy = correct_predictions / len(predicted_values)

# Print Test Results
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")
print(f"Average Test BLEU Score: {average_test_bleu:.4f}")

# Save the Final Model
save_directory = "./final_models/bart_model_updated"
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)
print(f"Model and tokenizer saved in {save_directory}")